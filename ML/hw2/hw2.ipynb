{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj_j1Ibc6376"
      },
      "source": [
        "# Домашнее задание 2 по ML\n",
        "Нужно написать python класс градиентного бустинга и побить другую модель на предоставленном baseline\n",
        "\n",
        "## Критерии оценки\n",
        "- Ваш ноутбук будет запущен через ```run all``` - он не должен упасть (допускается падение из-за отсутствия библиотеки, которую можно поставить через pip install)\n",
        "- Вот этот код (внизу ноутбука) ```assert imp_my_little_model > imp_baseline_model``` не вызвал ошибок (успешно отработал)\n",
        "\n",
        "- реализованы следующие гиперпараметры\n",
        "  - вы реализовали гиперпараметр ```learning_rate```\n",
        "  - вы реализовали гиперпараметр ```n_estimators```\n",
        "  - вы реализовали гиперпараметр ```max_depth```\n",
        "  - вы реализовали гиперпараметр ```bagging_fraction```\n",
        "\n",
        "- Вы реализовали [Huber loss function](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C_%D0%A5%D1%8C%D1%8E%D0%B1%D0%B5%D1%80%D0%B0) - она записана как отдельная def функция вне класса - и используется в вашем классе для расчета\n",
        "\n",
        "----\n",
        "*Для успешной сдачи дз нужно выполнить полностью каждый пункт выше*\n",
        "\n",
        "- оценка 5 будет поставлена, если каждый пункт выполнен без недочетов\n",
        "- оценка 4 будет поставлена, если будет найден один недочет\n",
        "- незачет будет пославлен, если недочетов будет два или более\n",
        "- незачет будет пославлен, если какой либо пункт не выполнен\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1YyKnF4T6e5Y",
        "outputId": "a6bce8ec-dc21-4cca-c1df-9746c7c0e2d5"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "IPython.notebook.set_autosave_interval(60000)"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fuou3iRh67G-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\coding\\python\\MTS\\ML\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# pip install shap\n",
        "\n",
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TSWaYEYyT0iV"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M4B4Ep5U68x5"
      },
      "outputs": [],
      "source": [
        "data, target = shap.datasets.california()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6-T1zHhWl69"
      },
      "source": [
        "### Не меняйте название для предскзаний preds_my_little_model, иначе не получится сдать это ДЗ (сломается код)\n",
        "\n",
        "Некоторые правила\n",
        "- Нельзя использовать никакие другие алгоритмы моделей внутри вашего класса, кроме DecisionTreeRegressor.\n",
        "- Код вашего бустинга должен быть написан в классе, у класса должно быть два ожидаемых метода : ```fit``` и ```predict```.\n",
        "- Нельзя менять датасет (и модифицировать тоже, например заполнять nan или применять scaler) или baseline модель\n",
        "- Нельзя поднимать число n_estimators вашей модели выше 100 (чтобы результат был сравним с моделью-конкурентом ```GradientBoostingRegressor```)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61wEAl-UIA4u"
      },
      "source": [
        "# *это место для вашего кода* ↓↓↓\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5ahK530JP-TV"
      },
      "outputs": [],
      "source": [
        "def huber(y_true, y_pred, reduction=None, delta=1):\n",
        "    res = y_true - y_pred\n",
        "    abs_res = np.abs(res)\n",
        "    loss = np.where(abs_res < delta, 0.5 * res**2, delta * (abs_res - 0.5 * delta))\n",
        "    if reduction == 'mean':\n",
        "        return np.mean(loss)\n",
        "    elif reduction == 'sum':\n",
        "        return np.sum(loss)\n",
        "    elif reduction == None:\n",
        "        return loss\n",
        "\n",
        "    return None\n",
        "\n",
        "def grad_huber(y_true, y_pred, delta=1):\n",
        "    res = y_true - y_pred\n",
        "    grad = np.where(np.abs(res) < delta, -res, -delta * np.sign(res))\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PPwM5ru6ITFw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class MyGradBoosting:\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        random_state=4,\n",
        "        bagging_fraction=0.75\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.bagging_fraction = bagging_fraction\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        self.trees = []\n",
        "\n",
        "        self.y_mean = np.mean(y)\n",
        "        y_pred = np.ones_like(y) * self.y_mean\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "\n",
        "            grad = -grad_huber(y, y_pred)\n",
        "\n",
        "            sample_indices = np.random.choice(range(n_samples), int(n_samples * self.bagging_fraction))\n",
        "            X_sample = X.iloc[sample_indices]\n",
        "            y_sample = grad[sample_indices]\n",
        "\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "            \n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.ones_like(X.shape[0]) * self.y_mean\n",
        "        for tree in self.trees:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jDMV0_KfBDEd"
      },
      "outputs": [],
      "source": [
        "my_little_model = MyGradBoosting()\n",
        "my_little_model.fit(X_train, y_train)\n",
        "\n",
        "preds_my_little_model = my_little_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CPzhitJHKdXV"
      },
      "outputs": [],
      "source": [
        "# самопроверки\n",
        "assert preds_my_little_model.shape == y_test.shape, 'что-то не так с выходным размером предикта'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XekwIQWHIKtE"
      },
      "source": [
        "# *это место для вашего кода* ↑↑↑"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrLezIsFHUP3"
      },
      "source": [
        "### *ниже ничего менять не нужно*\n",
        "## Это класс судья - он решит, какая модель оказалась лучше, ваша, или GradientBoostingRegressor из sklearn\n",
        "Если ячейка ниже завершилась ошибкой, нужно поменять код вашей модели и попробовать еще раз, до тех пор, пока не получите сообщение \"Ура, получилось!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5cwE3WwPBcKu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mape - ваша модель 0.19106502207812515\n",
            "mape - baseline 0.2152446498010688\n",
            "Ура, получилось! baseline важность: 0.097; важность вашей модели: 0.903\n"
          ]
        }
      ],
      "source": [
        "baseline_model = GradientBoostingRegressor(random_state=4, verbose=0)\n",
        "baseline_model = baseline_model.fit(X_train, y_train)\n",
        "preds_baseline_model = baseline_model.predict(X_test)\n",
        "print('mape - ваша модель', mean_absolute_percentage_error(y_test, preds_my_little_model))\n",
        "print('mape - baseline', mean_absolute_percentage_error(y_test, preds_baseline_model))\n",
        "\n",
        "final_estimator = RandomForestRegressor(random_state=16)\n",
        "final_estimator = final_estimator.fit(\n",
        "    np.hstack((preds_baseline_model.reshape(-1, 1), preds_my_little_model.reshape(-1, 1))),\n",
        "    y_test\n",
        ")\n",
        "\n",
        "imp_baseline_model, imp_my_little_model = final_estimator.feature_importances_\n",
        "result_message = f\"baseline важность: {imp_baseline_model:0.3f}; важность вашей модели: {imp_my_little_model:0.3f}\"\n",
        "\n",
        "assert imp_my_little_model > imp_baseline_model,  f'попробуй еще раз: {result_message}'\n",
        "print('Ура, получилось!',  result_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
